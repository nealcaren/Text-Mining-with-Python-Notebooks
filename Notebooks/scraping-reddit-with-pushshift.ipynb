{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Scraping Reddit with Pushshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lesson will walk you through the basics of scraping Reddit data using the popular Pushshift API.  We'll discuss some of the most useful parameters at our disposal, then work our way through a number of exercises putting those parameters to use.  Finally, we'll have some fun visualizing patterns in Reddit content. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reddit](https://www.reddit.com/) provides an additional avenue for large-scale digital data collection and analysis.\n",
    "\n",
    "\n",
    "Notably, Reddit allows its users, known as Redditors, to create and maintain communities, or \"subreddits,\" within the larger Reddit platform.  Redditors can opt in or out of a wide variety of communities at any time, allowing them to personalize their feeds and engage with a uniquely tailored online social circle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pushshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per the [subreddit](https://www.reddit.com/r/pushshift/comments/bcxguf/new_to_pushshift_read_this_faq/) dedicated to its discussion, pushshift.io allows users to analyze and aggregate large volumes of data from Reddit, while also providing the option to specify the desired ranges time from which to collect data.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous chapter discussing more general web scraping through APIs, we'll be importing both `requests` and `pandas`.  \n",
    "\n",
    "Additionally, we'll want to import `time`, `datetime`, and `sleep` to assist us with looping requests through time, and `matlibplot inline` to help with visualizing our scraped data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also want to save our base URL as a string for later use.  When using Pushshift, we'll have to indicate whether we want to scrape submissions or comments in our request within the base URL. To scrape submissions, the base URL should end with `/submission/`. To scrape comments, the base URL should end with `/comment/`.  So, if we want to request submissions we'd set our base URL as the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://api.pushshift.io/reddit/search/submission/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now discuss a number of the parameters available through Pushshift. You can find a full list of parameters available through the API [here](https://pushshift.io/api-parameters/).  We'll return to some of the parameters listed at the link provided later on in the chapter, when we begin analyzing some scraped Reddit content. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **sort**: Allows you to control the direction in which you scrape results. Results can be scraped in ascending ('asc') or descending ('desc') order. \n",
    "\n",
    "\n",
    "> - **sort_type**: Allows you to set the parameter used to sort requests. \n",
    "\n",
    "\n",
    "- **created_utc**: This parameter is useful for restricting requests by the Coordinated Universal Time, or UTC, of their creation.  There are a number of online conversion tools available for converting human-readable dates to UTCs; I typically use [this one](https://www.epochconverter.com/).\n",
    "\n",
    "\n",
    "> - **before**: Use this to scrape content created *before* a certain UTC.\n",
    "- **after**: Use this to scrape content created *after* a certain UTC.\n",
    "\n",
    "\n",
    "- **size**: Allows you to set a maximum size limit on your request.  Without looping requests, size per request maxes out at 1,000 submissions or comments.\n",
    "\n",
    "\n",
    "- **author**: Allows you to request only the content produced by a single Redditor, or a specific set of Redditors.\n",
    "\n",
    "\n",
    "- **subreddit**: Allows you to request only the content from a single subreddit, or a specific set of subreddits.\n",
    "\n",
    "\n",
    "- **score**: Allows you to sort or otherwise restrict requests depending on the net number of likes versus dislikes received by content.\n",
    "\n",
    "\n",
    "- **num_comments**: Allows you to sort or otherwise restrict requests depending on the total number of comments on submissions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pushshift Exercise 1: Making a Request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the API Documentation outlined above as a guide, let's use pushshift to scrape some Reddit content.  We'll start by looking at some submissions from [r/changemyview](https://www.reddit.com/r/changemyview/), which describes itself as \"a place to post an opinion you accept may be flawed, in an effort to understand other perspectives on the issue.\"\n",
    "\n",
    "\n",
    "Let's look at the 50 submissions to r/changemyview from 2019 that received the highest number of comments. We can start by setting the parameters for our request in a dictionary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'subreddit' : 'changemyview',\n",
    "              'sort'       : 'desc',\n",
    "              'sort_type'  : 'num_comments',\n",
    "              'after'       : '1546300800',\n",
    "              'before'       : '1577836800',\n",
    "              'size'       : 50,}     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to make our request to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(base_url, params = parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to store the data scraped in the request in JSON format, so we can access it within the notebook as a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(r.json()['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try looking at a random sample of 10 submissions from the request as a dataframe in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author_flair_text_color</th>\n",
       "      <th>author_flair_type</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>author_patreon_flair</th>\n",
       "      <th>can_mod_post</th>\n",
       "      <th>...</th>\n",
       "      <th>allow_live_comments</th>\n",
       "      <th>awarders</th>\n",
       "      <th>steward_reports</th>\n",
       "      <th>total_awards_received</th>\n",
       "      <th>author_premium</th>\n",
       "      <th>og_description</th>\n",
       "      <th>og_title</th>\n",
       "      <th>gilded</th>\n",
       "      <th>post_hint</th>\n",
       "      <th>preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>kinglax</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_5lo3i</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GAMpro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_ysnrw</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>self</td>\n",
       "      <td>{'enabled': False, 'images': [{'id': 'C9KSOp_b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>juul_pod</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_174yan</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RedUlster</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_4hnldl0n</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>gucci_sweatbands</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_1m2qo0k9</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>lawtonj</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>7∆</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_n3v7d</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Hey-I-Read-It</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_23pcrwbr</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Highlyemployable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_4p7iimbi</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Caesaroctopus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_prc43</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Cadent_Knave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_iz35j</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              author author_flair_background_color author_flair_css_class  \\\n",
       "26           kinglax                           NaN                   None   \n",
       "13            GAMpro                           NaN                   None   \n",
       "48          juul_pod                           NaN                   None   \n",
       "5          RedUlster                           NaN                   None   \n",
       "47  gucci_sweatbands                           NaN                   None   \n",
       "45           lawtonj                                                 None   \n",
       "34     Hey-I-Read-It                           NaN                   None   \n",
       "25  Highlyemployable                           NaN                   None   \n",
       "49     Caesaroctopus                           NaN                   None   \n",
       "33      Cadent_Knave                           NaN                   None   \n",
       "\n",
       "   author_flair_richtext author_flair_text author_flair_text_color  \\\n",
       "26                    []              None                     NaN   \n",
       "13                    []              None                     NaN   \n",
       "48                    []              None                     NaN   \n",
       "5                     []              None                     NaN   \n",
       "47                    []              None                     NaN   \n",
       "45                    []                7∆                    dark   \n",
       "34                    []              None                     NaN   \n",
       "25                    []              None                     NaN   \n",
       "49                    []              None                     NaN   \n",
       "33                    []              None                     NaN   \n",
       "\n",
       "   author_flair_type author_fullname  author_patreon_flair  can_mod_post  ...  \\\n",
       "26              text        t2_5lo3i                 False         False  ...   \n",
       "13              text        t2_ysnrw                 False         False  ...   \n",
       "48              text       t2_174yan                 False         False  ...   \n",
       "5               text     t2_4hnldl0n                 False         False  ...   \n",
       "47              text     t2_1m2qo0k9                 False         False  ...   \n",
       "45              text        t2_n3v7d                 False         False  ...   \n",
       "34              text     t2_23pcrwbr                 False         False  ...   \n",
       "25              text     t2_4p7iimbi                 False         False  ...   \n",
       "49              text        t2_prc43                 False         False  ...   \n",
       "33              text        t2_iz35j                 False         False  ...   \n",
       "\n",
       "    allow_live_comments  awarders steward_reports total_awards_received  \\\n",
       "26                  NaN       NaN             NaN                   NaN   \n",
       "13                False        []              []                   0.0   \n",
       "48                False        []              []                   0.0   \n",
       "5                 False        []              []                   0.0   \n",
       "47                False       NaN             NaN                   0.0   \n",
       "45                  NaN       NaN             NaN                   NaN   \n",
       "34                  NaN       NaN             NaN                   NaN   \n",
       "25                False        []              []                   0.0   \n",
       "49                False        []              []                   0.0   \n",
       "33                False        []              []                   0.0   \n",
       "\n",
       "   author_premium og_description  og_title  gilded  post_hint  \\\n",
       "26            NaN            NaN       NaN     1.0        NaN   \n",
       "13            NaN            NaN       NaN     NaN       self   \n",
       "48          False            NaN       NaN     NaN        NaN   \n",
       "5             NaN                              NaN        NaN   \n",
       "47            NaN            NaN       NaN     2.0        NaN   \n",
       "45            NaN            NaN       NaN     1.0        NaN   \n",
       "34            NaN            NaN       NaN     NaN        NaN   \n",
       "25            NaN                              1.0        NaN   \n",
       "49          False            NaN       NaN     NaN        NaN   \n",
       "33          False            NaN       NaN     NaN        NaN   \n",
       "\n",
       "                                              preview  \n",
       "26                                                NaN  \n",
       "13  {'enabled': False, 'images': [{'id': 'C9KSOp_b...  \n",
       "48                                                NaN  \n",
       "5                                                 NaN  \n",
       "47                                                NaN  \n",
       "45                                                NaN  \n",
       "34                                                NaN  \n",
       "25                                                NaN  \n",
       "49                                                NaN  \n",
       "33                                                NaN  \n",
       "\n",
       "[10 rows x 65 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot of information available!  It will be useful to determine a list of keys in the JSON file we'd like to look at, so we're not bombarded with too much information at one time.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author', 'author_flair_background_color', 'author_flair_css_class',\n",
       "       'author_flair_richtext', 'author_flair_text', 'author_flair_text_color',\n",
       "       'author_flair_type', 'author_fullname', 'author_patreon_flair',\n",
       "       'can_mod_post', 'contest_mode', 'created_utc', 'domain', 'full_link',\n",
       "       'gildings', 'id', 'is_crosspostable', 'is_meta', 'is_original_content',\n",
       "       'is_reddit_media_domain', 'is_robot_indexable', 'is_self', 'is_video',\n",
       "       'link_flair_background_color', 'link_flair_richtext',\n",
       "       'link_flair_text_color', 'link_flair_type', 'locked', 'media_only',\n",
       "       'no_follow', 'num_comments', 'num_crossposts', 'over_18',\n",
       "       'parent_whitelist_status', 'permalink', 'pinned', 'pwls',\n",
       "       'retrieved_on', 'score', 'selftext', 'send_replies', 'spoiler',\n",
       "       'stickied', 'subreddit', 'subreddit_id', 'subreddit_subscribers',\n",
       "       'subreddit_type', 'suggested_sort', 'thumbnail', 'title', 'updated_utc',\n",
       "       'url', 'whitelist_status', 'wls', 'all_awardings',\n",
       "       'allow_live_comments', 'awarders', 'steward_reports',\n",
       "       'total_awards_received', 'author_premium', 'og_description', 'og_title',\n",
       "       'gilded', 'post_hint', 'preview'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these keys (for example,`author`, `created_utc`, `num_comments`, and `score`) were outlined in the API definitions above, but a few of the keys are new.  Here are a couple of important keys we haven't yet discussed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **title**: This is the title a Redditor has given their submission.  \n",
    "\n",
    "\n",
    "- **selftext**: This is the text contained within the submission itself. While a submission will always contain some text in the `title`, some image-only submissions will not contain any text in `selftext`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better handle of the data we've scraped, let's narrow our focus to `author`, `num_comments`, `score`, `title`, and `selftext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['author', 'num_comments', 'score', 'title', 'selftext']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's pull another random sample of 10 submissions from the dataframe using our narrowed set of parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Birb-Brain-Syn</td>\n",
       "      <td>1468</td>\n",
       "      <td>3491</td>\n",
       "      <td>CMV: Anyone working a full-time job should be ...</td>\n",
       "      <td>So for me the above held opinion is straight-f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amiller1776</td>\n",
       "      <td>2617</td>\n",
       "      <td>1337</td>\n",
       "      <td>CMV: Trans activists who claim it is transphob...</td>\n",
       "      <td>Several trans activist youtubers have posted v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>BasicRedditor1997</td>\n",
       "      <td>1100</td>\n",
       "      <td>1</td>\n",
       "      <td>CMV:Making children pledge allegiance to the f...</td>\n",
       "      <td>Every day from Kindergarten through 12th grade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>mandi4910</td>\n",
       "      <td>1112</td>\n",
       "      <td>3046</td>\n",
       "      <td>CMV: Transgender women shouldn't be allowed to...</td>\n",
       "      <td>Ok first of all i really, really feel like a j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>psychoIogic</td>\n",
       "      <td>1254</td>\n",
       "      <td>1696</td>\n",
       "      <td>CMV: Circumcision of babies should be illegal ...</td>\n",
       "      <td>Some people believe there are benefits that wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Akukurotenshi</td>\n",
       "      <td>1783</td>\n",
       "      <td>1847</td>\n",
       "      <td>CMV: Eating dogs is not wrong</td>\n",
       "      <td>You always see people demeaning those who have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Insert_Alias_Heree</td>\n",
       "      <td>1377</td>\n",
       "      <td>2274</td>\n",
       "      <td>CMV: Those born in poor economic backgrounds, ...</td>\n",
       "      <td>With the increasing amount of Republican actio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>killgoresalmonman</td>\n",
       "      <td>1660</td>\n",
       "      <td>3262</td>\n",
       "      <td>CMV: There’s no fact or logic based reason to ...</td>\n",
       "      <td>The science is clear that climate change is re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ThrowingThisAwayBtw</td>\n",
       "      <td>1594</td>\n",
       "      <td>1</td>\n",
       "      <td>CMV: Killing animals for pleasure is wrong</td>\n",
       "      <td>Hi all!\\n\\nI've recently been thinking a lot a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>carlsaganheaven</td>\n",
       "      <td>1877</td>\n",
       "      <td>3062</td>\n",
       "      <td>CMV: In heterosexual relationships the problem...</td>\n",
       "      <td>It's a common conception that when you marry a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author  num_comments  score  \\\n",
       "15       Birb-Brain-Syn          1468   3491   \n",
       "0           Amiller1776          2617   1337   \n",
       "42    BasicRedditor1997          1100      1   \n",
       "41            mandi4910          1112   3046   \n",
       "30          psychoIogic          1254   1696   \n",
       "9         Akukurotenshi          1783   1847   \n",
       "20   Insert_Alias_Heree          1377   2274   \n",
       "11    killgoresalmonman          1660   3262   \n",
       "12  ThrowingThisAwayBtw          1594      1   \n",
       "6       carlsaganheaven          1877   3062   \n",
       "\n",
       "                                                title  \\\n",
       "15  CMV: Anyone working a full-time job should be ...   \n",
       "0   CMV: Trans activists who claim it is transphob...   \n",
       "42  CMV:Making children pledge allegiance to the f...   \n",
       "41  CMV: Transgender women shouldn't be allowed to...   \n",
       "30  CMV: Circumcision of babies should be illegal ...   \n",
       "9                       CMV: Eating dogs is not wrong   \n",
       "20  CMV: Those born in poor economic backgrounds, ...   \n",
       "11  CMV: There’s no fact or logic based reason to ...   \n",
       "12         CMV: Killing animals for pleasure is wrong   \n",
       "6   CMV: In heterosexual relationships the problem...   \n",
       "\n",
       "                                             selftext  \n",
       "15  So for me the above held opinion is straight-f...  \n",
       "0   Several trans activist youtubers have posted v...  \n",
       "42  Every day from Kindergarten through 12th grade...  \n",
       "41  Ok first of all i really, really feel like a j...  \n",
       "30  Some people believe there are benefits that wa...  \n",
       "9   You always see people demeaning those who have...  \n",
       "20  With the increasing amount of Republican actio...  \n",
       "11  The science is clear that climate change is re...  \n",
       "12  Hi all!\\n\\nI've recently been thinking a lot a...  \n",
       "6   It's a common conception that when you marry a...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[columns_to_keep].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a bit easier to handle.  We can always add or take away keys as needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pushshift Exercise 2: Looping Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll probably want to obtain more than fifty submissions and/or comments when conducting your own research projects. While you can obtain up to 1,000 submissions or comments per Pushshift request, most data analysis will require you to collect far more content. With that in mind, we'll now cover how we can loop requests using the Pushshift API. \n",
    "\n",
    "\n",
    "In this second exercise, we'll continue looking at r/changemyview.  This time, however, we'll attempt to collect *all* comments from the subreddit posted in January 2019. \n",
    "\n",
    "\n",
    "We can get started by setting the first and last UTCs we'll be making requests from. Since we're looking at all comments from January 2019, that makes our first UTC the beginning of the month, and the last UTC the end of the month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTC for 1/01/2019, 12:00:00 AM GMT\n",
    "\n",
    "min_utc = 1546300800\n",
    "\n",
    "# UTC for 1/31/2019, 11:59:59 PM GMT\n",
    "\n",
    "last_utc = 1548979199"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll want to define a function that allows us to collect a batch of 1,000 comments from r/changemyview. To get started, let's revise the base URL we defined earlier in the chapter so that Pushshift returns comments rather than submissions from r/changemyview. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://api.pushshift.io/reddit/search/comment/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define the parameters in place for each batch we'll be collecting. In this exercise, we'll begin requests are the start of January 2019 and work our way to the end of the month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "  parameters = {'subreddit' : 'changemyview',\n",
    "                'after'    : min_utc,\n",
    "              'sort'       : 'asc',\n",
    "              'sort_type'  : 'created_utc',\n",
    "              'size'       : 1000,}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there have almost certainly been more than 1,000 comments posted to r/changemyview over the course of 2019, we'll need to define a function that lets us update the first UTC from which to begin scraping as we move through our loop, and begins scraping with the maximum UTC from the previous batch of comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_utc(df):\n",
    "  newest_utc = df['created_utc'].values.max()\n",
    "  return newest_utc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By toying with the parameters and using the same base URL, we can also define functions returening the first and last UTCs for comments on r/changemymind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_utc(subreddit):\n",
    "  base_url =  'https://api.pushshift.io/reddit/search/comment/'\n",
    "\n",
    "  parameters = {'subreddit' : subreddit,\n",
    "              'sort'       : 'asc',\n",
    "              'sort_type'  : 'created_utc',\n",
    "              'size'       : 1,}\n",
    "              \n",
    "  df = requests.get(base_url, params = parameters)\n",
    "  df = pd.DataFrame(df.json()['data'])\n",
    "\n",
    "  return df['created_utc'].values.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_utc(subreddit):\n",
    "  base_url =  'https://api.pushshift.io/reddit/search/comment/'\n",
    "\n",
    "  parameters = {'subreddit' : subreddit,\n",
    "              'sort'       : 'desc',\n",
    "              'sort_type'  : 'created_utc',\n",
    "              'size'       : 1,}\n",
    "              \n",
    "  df = requests.get(base_url, params = parameters)\n",
    "  df = pd.DataFrame(df.json()['data'])\n",
    "  return find_max_utc(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should define our request function so that we always return a dataframe stored in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(subreddit, min_utc):\n",
    "  base_url =  'https://api.pushshift.io/reddit/search/comment/'\n",
    "\n",
    "  parameters = {'subreddit' : subreddit,\n",
    "                'after'    : min_utc,\n",
    "              'sort'       : 'asc',\n",
    "              'sort_type'  : 'created_utc',\n",
    "              'size'       : 1000,}\n",
    "\n",
    "  r = requests.get(base_url, params = parameters)\n",
    "  try:\n",
    "    df = pd.DataFrame(r.json()['data'])\n",
    "  except:\n",
    "    print(r.text)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altogether, our first set of defined functions should look like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_utc(df):\n",
    "    newest_utc = df['created_utc'].values.max()\n",
    "    return newest_utc\n",
    "\n",
    "\n",
    "def get_first_utc(subreddit):\n",
    "    base_url =  'https://api.pushshift.io/reddit/search/comment/'\n",
    "\n",
    "    parameters = {'subreddit' : subreddit,\n",
    "                 'sort'       : 'asc',\n",
    "                 'sort_type'  : 'created_utc',}\n",
    "    return newest_utc\n",
    "\n",
    "\n",
    "def get_first_utc(subreddit):\n",
    "  base_url =  'https://api.pushshift.io/reddit/search/comment/'\n",
    "\n",
    "  parameters = {'subreddit' : subreddit,\n",
    "              'sort'       : 'asc',\n",
    "              'sort_type'  : 'created_utc',\n",
    "              'size'       : 1,}\n",
    "              \n",
    "  df = requests.get(base_url, params = parameters)\n",
    "  df = pd.DataFrame(df.json()['data'])\n",
    "\n",
    "  return df['created_utc'].values.min()\n",
    "\n",
    "\n",
    "def get_last_utc(subreddit):\n",
    "  base_url =  'https://api.pushshift.io/reddit/search/comment/'\n",
    "\n",
    "  parameters = {'subreddit' : subreddit,\n",
    "              'sort'       : 'desc',\n",
    "              'sort_type'  : 'created_utc',\n",
    "              'size'       : 1,}\n",
    "              \n",
    "  df = requests.get(base_url, params = parameters)\n",
    "  df = pd.DataFrame(df.json()['data'])\n",
    "  return find_max_utc(df)\n",
    " \n",
    "\n",
    "def get_batch(subreddit, min_utc):\n",
    "  base_url =  'https://api.pushshift.io/reddit/search/comment/'\n",
    "\n",
    "  parameters = {'subreddit' : subreddit,\n",
    "                'after'    : min_utc,\n",
    "              'sort'       : 'asc',\n",
    "              'sort_type'  : 'created_utc',\n",
    "              'size'       : 1000,}\n",
    "\n",
    "  r = requests.get(base_url, params = parameters)\n",
    "  try:\n",
    "    df = pd.DataFrame(r.json()['data'])\n",
    "  except:\n",
    "    print(r.text)\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now walk through the how to define the function that will allow us to scrape all comments from r/changemyview from January 2019.\n",
    "\n",
    "Before we can begin finding maximum UTCs from previously requested batches of comments, we'll need to set a temporary starting UTC. For this exercise, the starting UTC will be the same as the first UTC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTC for 1/01/2019, 12:00:00 AM GMT\n",
    "\n",
    "max_utc = 1546300800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compile multiple batches of 1,000 comments into a list of dataframes, we'll also have to set up an empty list to hold all the batches we scrape.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to set up the loop itself.  We can construct a `while` loop to move backward through time and collect concurrent batches until we've reached the final UTC from which we'd like to make requests.\n",
    "\n",
    "We'll also want to make sure we append each batch of comments to our previously defined list as we move through the loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " while max_utc <= last_utc:\n",
    "\n",
    "    df = get_batch(subreddit, max_utc)\n",
    " \n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to use the previously defined `find_max_utc` function to allow our `while` loop to iteratively update the starting UTC as we request multiple batches. We can include an exception that allows us to concatenate the final batch of comments into our list of dataframes using the `concat` function from `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    max_utc = find_max_utc(df)\n",
    "except:\n",
    "    print('error with max_utc')\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to use `sleep` to avoid overloading the server as we make multiple requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `print` to view our loop function's progress in real time, and to ensure the function works. Importing `datetime` also lets us convert UTCs from the API into human readable dates, so that we can easily interpret feedback from the `print` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.utcfromtimestamp(max_utc).isoformat())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to apply the `concat` function from before to the entire function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altogether, our scraping function will look like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_comments(subreddit):\n",
    "  \n",
    "  min_utc = 1546300800\n",
    "  last_utc = 1548979199\n",
    "  \n",
    "  dfs = []\n",
    "  \n",
    "  max_utc = 1546300800\n",
    "\n",
    "  \n",
    "  while max_utc <= last_utc:\n",
    "    \n",
    "    df = get_batch(subreddit, max_utc)   \n",
    "    dfs.append(df)\n",
    "    \n",
    "    try:\n",
    "      max_utc = find_max_utc(df)\n",
    "    except:\n",
    "      print('error with max_utc')\n",
    "      return pd.concat(dfs)\n",
    "    \n",
    "    sleep(1)\n",
    "    \n",
    "    print(datetime.utcfromtimestamp(max_utc).isoformat())\n",
    "  \n",
    "  return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our definitions in place, let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-01T14:53:37\n",
      "2019-01-01T21:37:01\n",
      "2019-01-02T05:45:36\n",
      "2019-01-02T15:12:58\n",
      "2019-01-02T19:00:43\n",
      "2019-01-02T22:36:42\n",
      "2019-01-03T04:21:37\n",
      "2019-01-03T14:25:22\n",
      "2019-01-03T19:22:15\n",
      "2019-01-03T22:46:07\n",
      "2019-01-04T02:12:33\n",
      "2019-01-04T14:20:17\n",
      "2019-01-04T22:43:53\n",
      "2019-01-05T10:31:28\n",
      "2019-01-05T17:44:18\n",
      "2019-01-05T21:26:15\n",
      "2019-01-06T01:57:02\n",
      "2019-01-06T10:08:35\n",
      "2019-01-06T17:29:13\n",
      "2019-01-06T22:35:04\n",
      "2019-01-07T05:56:50\n",
      "2019-01-07T14:53:59\n",
      "2019-01-07T19:20:04\n",
      "2019-01-07T22:46:18\n",
      "2019-01-08T03:01:30\n",
      "2019-01-08T13:53:38\n",
      "2019-01-08T18:10:35\n",
      "2019-01-08T22:28:07\n",
      "2019-01-09T05:54:08\n",
      "2019-01-09T15:43:55\n",
      "2019-01-09T21:08:21\n",
      "2019-01-10T03:25:49\n",
      "2019-01-10T15:43:35\n",
      "2019-01-10T19:40:44\n",
      "2019-01-10T23:19:43\n",
      "2019-01-11T09:56:32\n",
      "2019-01-11T17:40:12\n",
      "2019-01-11T23:31:03\n",
      "2019-01-12T14:58:07\n",
      "2019-01-12T21:40:03\n",
      "2019-01-13T04:17:01\n",
      "2019-01-13T12:30:54\n",
      "2019-01-13T21:24:40\n",
      "2019-01-14T04:08:27\n",
      "2019-01-14T17:15:16\n",
      "2019-01-15T00:39:12\n",
      "2019-01-15T14:19:24\n",
      "2019-01-15T18:18:41\n",
      "2019-01-15T20:53:48\n",
      "2019-01-16T00:26:01\n",
      "2019-01-16T05:30:37\n",
      "2019-01-16T12:53:52\n",
      "2019-01-16T16:13:43\n",
      "2019-01-16T19:20:40\n",
      "2019-01-16T22:44:03\n",
      "2019-01-17T04:50:07\n",
      "2019-01-17T15:24:33\n",
      "2019-01-17T19:58:19\n",
      "2019-01-18T02:45:58\n",
      "2019-01-18T16:19:43\n",
      "2019-01-18T20:42:02\n",
      "2019-01-19T05:02:18\n",
      "2019-01-19T17:00:46\n",
      "2019-01-19T22:41:13\n",
      "2019-01-20T04:37:37\n",
      "2019-01-20T14:52:09\n",
      "2019-01-20T18:18:56\n",
      "2019-01-20T21:31:40\n",
      "2019-01-21T00:07:52\n",
      "2019-01-21T03:45:43\n",
      "2019-01-21T08:18:44\n",
      "2019-01-21T15:59:29\n",
      "2019-01-21T18:54:02\n",
      "2019-01-21T22:06:45\n",
      "2019-01-22T01:46:17\n",
      "2019-01-22T06:58:03\n",
      "2019-01-22T15:30:06\n",
      "2019-01-22T18:38:04\n",
      "2019-01-22T20:38:26\n",
      "2019-01-22T23:32:34\n",
      "2019-01-23T04:52:24\n",
      "2019-01-23T16:38:38\n",
      "2019-01-23T19:46:33\n",
      "2019-01-23T22:51:41\n",
      "2019-01-24T03:45:50\n",
      "2019-01-24T15:46:39\n",
      "2019-01-24T20:11:22\n",
      "2019-01-25T00:23:17\n",
      "2019-01-25T13:06:30\n",
      "2019-01-25T18:31:20\n",
      "2019-01-26T00:41:59\n",
      "2019-01-26T15:31:20\n",
      "2019-01-26T23:15:35\n",
      "2019-01-27T11:52:30\n",
      "2019-01-27T18:43:51\n",
      "2019-01-28T02:22:30\n",
      "2019-01-28T09:04:17\n",
      "2019-01-28T16:58:28\n",
      "2019-01-28T21:47:40\n",
      "2019-01-29T03:55:58\n",
      "2019-01-29T13:33:26\n",
      "2019-01-29T18:23:08\n",
      "2019-01-29T22:19:46\n",
      "2019-01-30T05:41:27\n",
      "2019-01-30T16:39:29\n",
      "2019-01-30T20:45:11\n",
      "2019-01-31T01:03:38\n",
      "2019-01-31T05:29:49\n",
      "2019-01-31T16:32:49\n",
      "2019-01-31T21:07:08\n",
      "2019-02-01T03:46:52\n"
     ]
    }
   ],
   "source": [
    "cmv_df = scrape_comments('changemyview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For relatively simple (but easily obtainable) data visualizations through the Pushift API, we can check out the [Pushshift Reddit Search](https://redditsearch.io/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
