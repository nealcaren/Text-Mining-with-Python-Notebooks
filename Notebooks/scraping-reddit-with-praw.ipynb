{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study, Part 2: Scraping Reddit with PRAW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although Pushshift is a wonderful resource when it comes to scraping Reddit data, it's not infallible. In some cases, important data will be missing from the Pushshift API, and you'll need to supplement the Pushshift data with the metadata available through Reddit's official API. \n",
    "\n",
    "Luckily, we can accomplish this using the [PRAW](https://praw.readthedocs.io/en/latest/) Reddit API Wrapper. This chapter will go through the steps necessary to supplement Pushshift data using PRAW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Reddit App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use PRAW, you'll need to develop your own application on Reddit. In order to do *that*, you'll need to create a Reddit account. \n",
    "\n",
    "\n",
    "Once you've created an account on Reddit, you can navigate to the [developed applications](https://www.reddit.com/prefs/apps) page from Reddit preferences. Here, you'll see a button prompting you to \"create app.\" Click it, and you should see the following: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![create application](https://i.snipboard.io/zKZ3vq.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you're creating a **script** app, as this is what we'll need in order to make requests with PRAW. Feel free to name and describe the app as you see fit, then click the button at the bottom to create your app. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional guidance on how to develop your own Reddit application, see [here](https://github.com/reddit-archive/reddit/wiki/OAuth2-Quick-Start-Example#first-steps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining a Reddit Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created an application on Reddit, we can obtain Reddit instances using PRAW. While it's possible to create two separate types of instance -- read-only or authorized -- for the sake of this chapter we'll focus on obtaining a read-only instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the script application we just created becomes important. We'll need to provide our `client-id`, our `client_secret`, and our `user_agent` in order to obtain a read-only Reddit instance:\n",
    "\n",
    "*Note*: You'll want to keep this information as confidential as possible while still accessing the data you need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=\"my client id\",\n",
    "                     client_secret=\"my client secret\",\n",
    "                     user_agent=\"my user agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the information from our script application, we'll be able to `print` a read-only Reddit instance. As with Pushshift, we'll have to determine whether we'd like to look at data for submissions or comments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, below are the 3 \"hottest\" submissions from [r/MakeupRehab](https://www.reddit.com/r/MakeupRehab/), along with submission authors, titles, scores, and body text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "toyaqueen\n",
      "Megathread: COVID-19 / Coronavirus Resource and Discussion\n",
      "43\n",
      "Hi everyone,\n",
      "\n",
      "r/DISCLAIMER: This is not to be taken as a post for health information or precautions on the novel coronavirus pandemic. PLEASE keep up-to-date via your local governments and health representatives.\n",
      "\n",
      "As we all know by now, the novel coronavirus is impacting daily life all over the globe. Here in MUR, you may have already seen some of our fellow members post about struggles directly related to the virus - having to move suddenly and unexpectedly, job losses, trying to deal with the stress, seeing which companies are being shady and more. With that in mind, feel free to use this thread as the hub for any conversations you want to have on this topic.\n",
      "\n",
      "We’ve decided to create a megathread that will be updated with resources from other subs (the crosspost rule will be lifted for the purposes of this thread only), relevant to the needs and interests of the community to use during this trying time for many of us.  If you see more, comment below and I will edit this to add if appropriate.\n",
      "\n",
      "Legal/Finance Resources:\n",
      "\n",
      "r/personalfinance:  [Coronavirus Megathread](https://www.reddit.com/r/personalfinance/comments/fhrfqo/coronavirus_megathread_resources_discussion_and/), [Job Loss Megathread](https://www.reddit.com/r/personalfinance/comments/fkyu8h/job_loss_megathread_unemployment_resources/)\n",
      "\n",
      "r/legaladvice: [FAQ](https://www.reddit.com/r/legaladvice/comments/fgpvcv/covid19_faq_a_workinprogress/)\n",
      "\n",
      "Beauty Companies Status:\n",
      "\n",
      "r/MakeupAddiction: [Ulta Closes](https://www.reddit.com/r/MakeupAddiction/comments/fkepbx/ulta_announces_that_all_locations_will_be_closing/)\n",
      "\n",
      "r/Sephora: [SiJCP Remain Open](https://www.reddit.com/r/Sephora/comments/fk0ecr/remember_your_sijcps/); [SiJCP Employee Info](https://www.reddit.com/r/Sephora/comments/fkfm2t/sijcp_employe_info/); [Sephora Closing](https://www.reddit.com/r/Sephora/comments/fjurja/effective_tomorrow_at_5pm_all_sephora_stores_will/)\n",
      "\n",
      "r/Makeup: [Sephora Closing](https://www.reddit.com/r/Makeup/comments/fjyscd/update_sephora_closing_all_retail_stores/)\n",
      "\n",
      "r/muacjdiscussion: [ColourPop Shipping Center Closing](https://www.reddit.com/r/muacjdiscussion/comments/fnbg1k/colourpop_shipping_center_closed/);  [3/30/20 Update - ULTA Stores will remain closed](https://www.reddit.com/r/muacjdiscussion/comments/fruxhb/ulta_stores_will_continue_to_remain_closed/)\n",
      "\n",
      "[TJX Companies (TJ Maxx, Marshall's, etc) COVID-19 Closure Announcement](https://www.tjx.com/docs/default-source/default-document-library/a-message-from-the-tjx-companies-inc.pdf) \\[If you see discussions on Reddit from employees, please share!\\]\n",
      "\n",
      "Mental Health Resources:\n",
      "\n",
      "r/BPD: [Covid-19 Megathread](https://www.reddit.com/r/BPD/comments/fkvejt/covid19_megathread/)\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I will also use the comments in this thread to create specific quarantine related activities that we can do together while we’re under quarantine/isolation/distancing beginning tomorrow, Saturday the 21st - if you have anything you want to see, feel free to comment! We’ve always helped each other out in this community, and this virus has not changed that.\n",
      "\n",
      "\\-MUR Mod Team\n",
      "AutoModerator\n",
      "Willpower Wednesday - May 27, 2020\n",
      "5\n",
      "This thread is to keep each other grounded as we pursue low-buy or no-buy goals. The temptation is strong, so exercise your willpower with your fellow members!\n",
      "coffeeczar\n",
      "I stopped wearing foundation!\n",
      "196\n",
      "I  was obsessed with finding my HG foundation and spending too much money at Sephora/Ulta trying to find my perfect match. I've always struggled with acne since puberty (and now have adult acne and use tret but still have breakouts at 31).\n",
      "\n",
      "Since stay at home, I stopped wearing foundation and use my tinted CeraVe Mineral SPF as my moisturizer, sunscreen, and sheer wash of color all in one. And my acne? IT'S GONE! Prior to this, I was using Shiseido Syncro Skin, so I ran the ingredients on CosDNA and it has an ingredient 5 out of 5 on the comedogenic scale! No wonder I had so many breakouts this winter!  This prompted me to check my other foundations. My summer foundation (Smashbox Hydrating 15Hr) also has a comedogenic ingredient.  So I had to toss those two foundations. Everytime I get an itch to purchase a foundation I check CosDNA and always find a problematic ingredient (either fragrance or pore clogging).\n",
      "\n",
      "I will now only use non-comedogenic tinted mineral SPF. My skin has never looked better and my wallet is happier too :) I found my Maybelline AgeRewind works wonders as a foundation for the special occasions I might want more coverage.\n",
      "\n",
      "I'm so glad stay at home allowed me to make this discovery!\n"
     ]
    }
   ],
   "source": [
    "print(reddit.read_only)\n",
    "for submission in reddit.subreddit(\"MakeupRehab\").hot(limit=3):\n",
    "    print(submission.author)\n",
    "    print(submission.title)\n",
    "    print(submission.score)  \n",
    "    print(submission.selftext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: PRAW will not allow you to create instances for quarantined or banned subreddits. Attempting to do so will return 403 and 404 Errors, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in gaps in the Pushshift API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Pushshift is a great entryway into scraping data from Reddit, you'll occasionally run into some notable gaps in available data.\n",
    "\n",
    "Using the API's [Reddit Search](https://redditsearch.io/) referenced at the end of Part 1, we can see how scope out any potential holes in subreddit data before going through the process of scraping that data ourselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's look at [r/AmItheAsshole](https://www.reddit.com/r/AmItheAsshole/):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![r/AmItheAsshole](https://i.snipboard.io/BwiqKY.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data visualizations available through Pushshift stand in pretty stark contrast to r/AmItheAsshole's creation in June 2013. As previously demonstrated by [Elle O'Brien](https://dvc.org/blog/a-public-reddit-dataset), however, we should be able to use PRAW to obtain the missing data prior to 2019.\n",
    "\n",
    "\n",
    "Below is a step-by-step guide of how to supplement missing Pushshift data using PRAW, using submissions from r/AmItheAsshole as an example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Compiling Submission ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step will be quite similar to what we've done to collect comments from r/changemyview over the first week of 2019.  As we did there, we'll be defining a `while` loop to iteratively scrape subreddit data through time. In this exercise, we'll focus on collecting submissions from [r/AmItheAsshole](https://www.reddit.com/r/AmItheAsshole/). However, since Pushshift has some holes in the data from this subreddit, we'll be putting together a far less substantial dataframe. Here, we'll only be interested in putting together a list of submission ids and timestamps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start things off by setting our minimum and final UTC. We'll want to start prior with the subreddit's creation of June 8th, 2013. For now, we'll set the `last_UTC` to the beginning of 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_UTC = 1370649600 # June 08, 2013 12:00:00 AM\n",
    "last_UTC = 1577836800 # January 1, 2020 12:00:00 AM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define our `url` and `parameters` so we can make requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'before' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-1d5afd7d1b4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://api.pushshift.io/reddit/submission/search/?sort_type=created_utc&sort=asc&subreddit=amitheasshole&after='\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mafter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\"&before\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbefore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"&size=1000\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'before' is not defined"
     ]
    }
   ],
   "source": [
    "    url = 'https://api.pushshift.io/reddit/submission/search/?sort_type=created_utc&sort=asc&subreddit=amitheasshole&after='+ str(after) +\"&before\"+str(before)+\"&size=1000\"\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we'll want to put together a handful of empty lists to store our data. This time around, we'll make separate lists for `timestamps`, `post_ids`, and `score`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = list()\n",
    "post_ids = list()\n",
    "score = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll set up our `while` loop. We'll append the `id` and `created_utc` keys to the empty lists we've prepared. \n",
    "\n",
    "Note: The string we `print` at the bottom lets us see how many posts we've scraped up to a certain point. We also made sure to let the API `sleep` for 1/10th of a second before moving on to the next iteration of the loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after = min_UTC\n",
    "while int(after) < last_UTC:\n",
    "    data = getPushshiftData(after,last_UTC)\n",
    "    for post in data:\n",
    "        tmp_time = post['created_utc']\n",
    "        tmp_id = post['id']\n",
    "        timestamps.append(tmp_time)\n",
    "        post_ids.append(tmp_id)\n",
    "    after = timestamps[-1]\n",
    "    print([str(len(post_ids)) + \" posts collected so far.\"])\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def getPushshiftData(after, before):\n",
    "    \n",
    "    min_utc = 1370649600\n",
    "    last_utc = 1577836800\n",
    "    \n",
    "    url = 'https://api.pushshift.io/reddit/submission/search/?sort_type=created_utc&sort=asc&subreddit=amitheasshole&after='+ str(after) +\"&before\"+str(before)+\"&size=1000\"\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']\n",
    "\n",
    "\n",
    "    timestamps = list()\n",
    "    post_ids = list()\n",
    "    score = list()\n",
    "\n",
    "after = min_UTC\n",
    "while int(after) < last_UTC:\n",
    "    data = getPushshiftData(after,last_UTC)\n",
    "    for post in data:\n",
    "        tmp_time = post['created_utc']\n",
    "        tmp_id = post['id']\n",
    "        timestamps.append(tmp_time)\n",
    "        post_ids.append(tmp_id)\n",
    "    after = timestamps[-1]\n",
    "    print([str(len(post_ids)) + \" posts collected so far.\"])\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create and save separate csv files for post ids and created UTCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'id':post_ids, 'timestamp':timestamps}\n",
    "df = pd.DataFrame(d)\n",
    "df.to_csv(\"post_ids.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"post_ids.csv\")\n",
    "use_posts = df['id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1fy0bx</td>\n",
       "      <td>1370724175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ytr72</td>\n",
       "      <td>1393275159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1ytxov</td>\n",
       "      <td>1393278651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1yu29c</td>\n",
       "      <td>1393281184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1yu41e</td>\n",
       "      <td>1393282238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354995</th>\n",
       "      <td>eilsld</td>\n",
       "      <td>1577903855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354996</th>\n",
       "      <td>eilsse</td>\n",
       "      <td>1577903883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354997</th>\n",
       "      <td>eiludm</td>\n",
       "      <td>1577904080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354998</th>\n",
       "      <td>eilv43</td>\n",
       "      <td>1577904171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354999</th>\n",
       "      <td>eilv4l</td>\n",
       "      <td>1577904172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>355000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id   timestamp\n",
       "0       1fy0bx  1370724175\n",
       "1       1ytr72  1393275159\n",
       "2       1ytxov  1393278651\n",
       "3       1yu29c  1393281184\n",
       "4       1yu41e  1393282238\n",
       "...        ...         ...\n",
       "354995  eilsld  1577903855\n",
       "354996  eilsse  1577903883\n",
       "354997  eiludm  1577904080\n",
       "354998  eilv43  1577904171\n",
       "354999  eilv4l  1577904172\n",
       "\n",
       "[355000 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355000\n"
     ]
    }
   ],
   "source": [
    "print(len(use_posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a datetime value counts plot with `matplotlib` gives us something similar to what we saw using the Reddit Search earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                  355000\n",
       "unique                 352989\n",
       "top       2019-05-15 15:34:28\n",
       "freq                        3\n",
       "first     2013-06-08 20:42:55\n",
       "last      2020-01-01 18:42:52\n",
       "Name: timestamp, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['timestamp'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x127da4760>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['timestamp'].dt.date.value_counts().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Scraping with PRAW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a list of post ids from the dataframe we created in step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_posts = df['id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make sure we're requesting a read-only Reddit instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit.read_only = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355000\n"
     ]
    }
   ],
   "source": [
    "print(len(use_posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('subreddit_posts_out.csv',\"w\") \n",
    "writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "header = [\"id\",\"timestamp\",\"title\",\"body\",\"edited\",\"verdict\",\"score\",\"num_comments\"]\n",
    "writer.writerow(header)\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for idx in use_posts:\n",
    "    post = reddit.submission(idx)\n",
    "\n",
    "    score = post.score\n",
    "    \n",
    "    if score >= 3:\n",
    "        title = post.title\n",
    "        body = post.selftext\n",
    "        edited = str(post.edited)\n",
    "        num_comments = post.num_comments\n",
    "        timestamp = post.created_utc\n",
    "        verdict = post.link_flair_text \n",
    "        if not verdict:\n",
    "            verdict =  \"NA\" \n",
    "            \n",
    "        line_stuff = [idx,timestamp,title,body,edited,verdict,score,num_comments]\n",
    "        writer.writerow(line_stuff)\n",
    "    else:\n",
    "        line_stuff = [idx,\"NA\",\"NA\",\"NA\",\"NA\",\"NA\",score,\"NA\"]\n",
    "        writer.writerow(line_stuff)\n",
    "\n",
    "    if counter % 1000 == 0:\n",
    "        print(counter)\n",
    "    counter += 1\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Cleaning and Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a function to clean the data we've collected. We'll start by useing the `lower` method on the `verdict` key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['verdict'] = df['verdict'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use the `replace` method to add some consistency to our verdicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['verdict'] = df['verdict'].str.replace(\"a--hole|a-hole\",\"asshole\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a `valid_list` so that our dataset only contains submissions that received valid verdicts.  We'll also use `replace` to remove any edits that could potentially spoil the verdict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_list = [\"asshole\",\"not the asshole\",\"everyone sucks\",\"no assholes here\"]\n",
    "df_use = df[df['verdict'].isin(valid_list)]\n",
    "df_use['body'] = df_use['body'].str.replace(\"(edit|update).*?(YTA|a-|ass|\\\\sta\\\\s)(.*)\",\"\",case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate a `gone_list` to get rid of posts that have been removed or deleted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gone_list = [\"[deleted]\",\"[removed]\",\"\"]\n",
    "df_use = df_use[df_use['body'].isin(gone_list)==False]\n",
    "print(\"After removing deleted posts, there are \" +  str(len(df_use)) + \" posts left.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since r/AmItheAsshole has four potential verdicts, we'll convert these into a binary variable.  Here, \"asshole\" and \"everyone sucks here\" will both be considered `is_asshole`, while all other verdicts will not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_use[\"is_asshole\"] = [1 if x in [\"asshole\",\"everyone sucks\"] else 0 for x in df_use[\"verdict\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altogether, our `clean_scrape` function will look like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_scrape(df):\n",
    "\n",
    "    df['verdict'] = df['verdict'].str.lower()\n",
    "\n",
    "    df['verdict'] = df['verdict'].str.replace(\"a--hole|a-hole\",\"asshole\") \n",
    "\n",
    "    valid_list = [\"asshole\",\"not the asshole\",\"everyone sucks\",\"no assholes here\"]\n",
    "    \n",
    "    df_use = df[df['verdict'].isin(valid_list)]\n",
    "    \n",
    "    df_use['body'] = df_use['body'].str.replace(\"(edit|update).*?(YTA|a-|ass|\\\\sta\\\\s)(.*)\",\"\",case=False)\n",
    "\n",
    "    gone_list = [\"[deleted]\",\"[removed]\",\"\"]\n",
    "    \n",
    "    df_use = df_use[df_use['body'].isin(gone_list)==False]\n",
    "    \n",
    "    print(\"After removing deleted posts, there are \" +  str(len(df_use)) + \" posts left.\")\n",
    "    \n",
    "    df_use[\"is_asshole\"] = [1 if x in [\"asshole\",\"everyone sucks\"] else 0 for x in df_use[\"verdict\"]]\n",
    "\n",
    "    return(df_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `concat` method from pandas to merge our old and new datasets.  We'll `drop_duplicates` from the concatenated dataset as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_scrape(old, new):\n",
    "    # old and new are pandas dataframes. should have the same columns\n",
    "    old = pd.concat([old,new])\n",
    "    old = old.drop_duplicates()\n",
    "    return(old)\n",
    "\n",
    "raw = pd.read_csv(\"aita_raw.csv\")\n",
    "grand = clean_scrape(raw)\n",
    "\n",
    "print(\"There are now \" +  str(len(grand)) + \" cleaned posts.\")\n",
    "\n",
    "grand.to_csv(\"aita_clean.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll use our `clean_scrape` function on the newly concatenated datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv(\"aita_raw.csv\")\n",
    "grand = clean_scrape(raw)\n",
    "\n",
    "print(\"There are now \" +  str(len(grand)) + \" cleaned posts.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
