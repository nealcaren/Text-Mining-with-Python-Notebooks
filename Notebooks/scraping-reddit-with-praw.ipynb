{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study, Part 2: Scraping Reddit with PRAW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although Pushshift is a wonderful resource when it comes to scraping Reddit data, it's not infallible. In some cases, important data will be missing from the Pushshift API, and you'll need to supplement the Pushshift data with the metadata available through Reddit's official API. \n",
    "\n",
    "Luckily, we can accomplish this using the [PRAW](https://praw.readthedocs.io/en/latest/) Reddit API Wrapper. This chapter will go through the steps necessary to supplement Pushshift data using PRAW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in /opt/anaconda3/envs/tmwp/lib/python3.8/site-packages (7.0.0)\n",
      "Requirement already satisfied: update-checker>=0.16 in /opt/anaconda3/envs/tmwp/lib/python3.8/site-packages (from praw) (0.17)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /opt/anaconda3/envs/tmwp/lib/python3.8/site-packages (from praw) (0.57.0)\n",
      "Requirement already satisfied: prawcore<2.0,>=1.3.0 in /opt/anaconda3/envs/tmwp/lib/python3.8/site-packages (from praw) (1.4.0)\n",
      "Requirement already satisfied: requests>=2.3.0 in /opt/anaconda3/envs/tmwp/lib/python3.8/site-packages (from update-checker>=0.16->praw) (2.23.0)\n",
      "Requirement already satisfied: six in /opt/anaconda3/envs/tmwp/lib/python3.8/site-packages (from websocket-client>=0.54.0->praw) (1.15.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/envs/tmwp/lib/python3.8/site-packages (from requests>=2.3.0->update-checker>=0.16->praw) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/envs/tmwp/lib/python3.8/site-packages (from requests>=2.3.0->update-checker>=0.16->praw) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/tmwp/lib/python3.8/site-packages (from requests>=2.3.0->update-checker>=0.16->praw) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda3/envs/tmwp/lib/python3.8/site-packages (from requests>=2.3.0->update-checker>=0.16->praw) (3.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Reddit App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use PRAW, you'll need to develop your own application on Reddit. In order to do *that*, you'll need to create a Reddit account. \n",
    "\n",
    "\n",
    "Once you've created an account on Reddit, you can navigate to the [developed applications](https://www.reddit.com/prefs/apps) page from Reddit preferences. Here, you'll see a button prompting you to \"create app.\" Click it, and you should see the following: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![create application](https://i.snipboard.io/zKZ3vq.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you're creating a **script** app, as this is what we'll need in order to make requests with PRAW. Feel free to name and describe the app as you see fit, then click the button at the bottom to create your app. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional guidance on how to develop your own Reddit application, see [here](https://github.com/reddit-archive/reddit/wiki/OAuth2-Quick-Start-Example#first-steps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining a Reddit Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created an application on Reddit, we can obtain Reddit instances using PRAW. While it's possible to create two separate types of instance -- read-only or authorized -- for the sake of this chapter we'll focus on obtaining a read-only instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the script application we just created becomes important. We'll need to provide our `client-id`, our `client_secret`, and our `user_agent` in order to obtain a read-only Reddit instance:\n",
    "\n",
    "*Note*: You'll want to keep this information as confidential as possible while still accessing the data you need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=\"my client id\",\n",
    "                     client_secret=\"my client secret\",\n",
    "                     user_agent=\"my user agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the information from our script application, we'll be able to `print` a read-only Reddit instance. As with Pushshift, we'll have to determine whether we'd like to look at data for submissions or comments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, below are the 3 \"hottest\" submissions from [r/MakeupRehab](https://www.reddit.com/r/MakeupRehab/), along with submission authors, titles, scores, and body text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "toyaqueen\n",
      "Megathread: COVID-19 / Coronavirus Resource and Discussion\n",
      "40\n",
      "Hi everyone,\n",
      "\n",
      "r/DISCLAIMER: This is not to be taken as a post for health information or precautions on the novel coronavirus pandemic. PLEASE keep up-to-date via your local governments and health representatives.\n",
      "\n",
      "As we all know by now, the novel coronavirus is impacting daily life all over the globe. Here in MUR, you may have already seen some of our fellow members post about struggles directly related to the virus - having to move suddenly and unexpectedly, job losses, trying to deal with the stress, seeing which companies are being shady and more. With that in mind, feel free to use this thread as the hub for any conversations you want to have on this topic.\n",
      "\n",
      "We’ve decided to create a megathread that will be updated with resources from other subs (the crosspost rule will be lifted for the purposes of this thread only), relevant to the needs and interests of the community to use during this trying time for many of us.  If you see more, comment below and I will edit this to add if appropriate.\n",
      "\n",
      "Legal/Finance Resources:\n",
      "\n",
      "r/personalfinance:  [Coronavirus Megathread](https://www.reddit.com/r/personalfinance/comments/fhrfqo/coronavirus_megathread_resources_discussion_and/), [Job Loss Megathread](https://www.reddit.com/r/personalfinance/comments/fkyu8h/job_loss_megathread_unemployment_resources/)\n",
      "\n",
      "r/legaladvice: [FAQ](https://www.reddit.com/r/legaladvice/comments/fgpvcv/covid19_faq_a_workinprogress/)\n",
      "\n",
      "Beauty Companies Status:\n",
      "\n",
      "r/MakeupAddiction: [Ulta Closes](https://www.reddit.com/r/MakeupAddiction/comments/fkepbx/ulta_announces_that_all_locations_will_be_closing/)\n",
      "\n",
      "r/Sephora: [SiJCP Remain Open](https://www.reddit.com/r/Sephora/comments/fk0ecr/remember_your_sijcps/); [SiJCP Employee Info](https://www.reddit.com/r/Sephora/comments/fkfm2t/sijcp_employe_info/); [Sephora Closing](https://www.reddit.com/r/Sephora/comments/fjurja/effective_tomorrow_at_5pm_all_sephora_stores_will/)\n",
      "\n",
      "r/Makeup: [Sephora Closing](https://www.reddit.com/r/Makeup/comments/fjyscd/update_sephora_closing_all_retail_stores/)\n",
      "\n",
      "r/muacjdiscussion: [ColourPop Shipping Center Closing](https://www.reddit.com/r/muacjdiscussion/comments/fnbg1k/colourpop_shipping_center_closed/);  [3/30/20 Update - ULTA Stores will remain closed](https://www.reddit.com/r/muacjdiscussion/comments/fruxhb/ulta_stores_will_continue_to_remain_closed/)\n",
      "\n",
      "[TJX Companies (TJ Maxx, Marshall's, etc) COVID-19 Closure Announcement](https://www.tjx.com/docs/default-source/default-document-library/a-message-from-the-tjx-companies-inc.pdf) \\[If you see discussions on Reddit from employees, please share!\\]\n",
      "\n",
      "Mental Health Resources:\n",
      "\n",
      "r/BPD: [Covid-19 Megathread](https://www.reddit.com/r/BPD/comments/fkvejt/covid19_megathread/)\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I will also use the comments in this thread to create specific quarantine related activities that we can do together while we’re under quarantine/isolation/distancing beginning tomorrow, Saturday the 21st - if you have anything you want to see, feel free to comment! We’ve always helped each other out in this community, and this virus has not changed that.\n",
      "\n",
      "\\-MUR Mod Team\n",
      "AutoModerator\n",
      "Willpower Wednesday - June 17, 2020\n",
      "8\n",
      "This thread is to keep each other grounded as we pursue low-buy or no-buy goals. The temptation is strong, so exercise your willpower with your fellow members!\n",
      "itchyitchiford\n",
      "I have finally come to terms with my fantasy self and I’m decluttering my high end palettes\n",
      "108\n",
      "For a couple years now I have been trying to declutter or use up my unnecessarily large makeup collection so that I actually use everything I own. I’ve been doing well but the two items I haven’t been able to part with are my UD Naked2 palette and my KVD Shade and Light eye palette. Both palettes are neutrals that I theoretically would use but in practice I always get turned off of them. I have sensitive eyes and it seems like the KVD formula doesn’t agree with me anymore, and I like the Naked2 palette but I’ve realized that really shimmery shades don’t suit me. Both are quite old and almost definitely expired. \n",
      "\n",
      "\n",
      "I recently bought a small Marcelle quad that has a couple subtle satin neutral shades and it’s made me feel like I can let go of these other palettes. No, it’s not as pigmented as the high end palettes but I’ve realized I don’t need super highly pigmented eyeshadows. I still have my UD Naked Basics2 palette which I love to death and have hit pan on. I will repurchase that palette when I have panned it completely. My only palettes are now the Marcelle quad and Naked Basics2, and I feel completely satisfied. I have about 6 other single shadows in a magnetic palette and that is my entire collection. \n",
      "\n",
      "\n",
      "Guys, I feel completely rehabbed. I have finally realized what I like to wear and what I actually will use. I own those things and have no desire to buy more. I still pick up the odd lip product if I use one up, but otherwise I am only replacing my basics. My whole collection fits in one small drawer with room to spare, and I don’t feel guilty about the money I have spent on beauty. Reading through this sub has been really helpful for me to get this far and I thank you all!\n"
     ]
    }
   ],
   "source": [
    "print(reddit.read_only)\n",
    "for submission in reddit.subreddit(\"MakeupRehab\").hot(limit=3):\n",
    "    print(submission.author)\n",
    "    print(submission.title)\n",
    "    print(submission.score)  \n",
    "    print(submission.selftext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: PRAW will not allow you to create instances for quarantined or banned subreddits. Attempting to do so will return 403 and 404 Errors, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in gaps in the Pushshift API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Pushshift is a great entryway into scraping data from Reddit, you'll occasionally run into some notable gaps in available data.\n",
    "\n",
    "Using the API's [Reddit Search](https://redditsearch.io/) referenced at the end of Part 1, we can see how scope out any potential holes in subreddit data before going through the process of scraping that data ourselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's look at [r/AmItheAsshole](https://www.reddit.com/r/AmItheAsshole/):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![r/AmItheAsshole](https://i.snipboard.io/BwiqKY.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data visualizations available through Pushshift stand in pretty stark contrast to r/AmItheAsshole's creation in June 2013. As previously demonstrated by [Elle O'Brien](https://dvc.org/blog/a-public-reddit-dataset), however, we should be able to use PRAW to obtain the missing data prior to 2019.\n",
    "\n",
    "\n",
    "Below is a step-by-step guide of how to supplement missing Pushshift data using PRAW, using submissions from r/AmItheAsshole as an example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Compiling Submission ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step will be quite similar to what we've done to collect comments from r/changemyview over the first week of 2019.  As we did there, we'll be defining a `while` loop to iteratively scrape subreddit data through time. In this exercise, we'll focus on collecting submissions from [r/AmItheAsshole](https://www.reddit.com/r/AmItheAsshole/). However, since Pushshift has some holes in the data from this subreddit, we'll be putting together a far less substantial dataframe. Here, we'll only be interested in putting together a list of submission ids and timestamps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start things off by setting our minimum and final UTC. We'll want to start prior with the subreddit's creation of June 8th, 2013. For now, we'll set the `last_UTC` to the beginning of 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_UTC = 1370649600 # June 08, 2013 12:00:00 AM\n",
    "last_UTC = 1577836800 # January 1, 2020 12:00:00 AM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define our `url` and `parameters` so we can make requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    url = 'https://api.pushshift.io/reddit/submission/search/?sort_type=created_utc&sort=asc&subreddit=amitheasshole&after='+ str(after) +\"&before\"+str(before)+\"&size=1000\"\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we'll want to put together a handful of empty lists to store our data. This time around, we'll make separate lists for `timestamps`, `post_ids`, and `score`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = []\n",
    "post_ids = []\n",
    "score = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll set up our `while` loop. We'll append the `id` and `created_utc` keys to the empty lists we've prepared. \n",
    "\n",
    "Note: The string we `print` at the bottom lets us see how many posts we've scraped up to a certain point. We also made sure to let the API `sleep` for 1/10th of a second before moving on to the next iteration of the loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after = min_UTC\n",
    "while int(after) < last_UTC:\n",
    "    data = getPushshiftData(after,last_UTC)\n",
    "    for post in data:\n",
    "        tmp_time = post['created_utc']\n",
    "        tmp_id = post['id']\n",
    "        timestamps.append(tmp_time)\n",
    "        post_ids.append(tmp_id)\n",
    "    after = timestamps[-1]\n",
    "    print([str(len(post_ids)) + \" posts collected so far.\"])\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def getPushshiftData(after, before):\n",
    "    url = 'https://api.pushshift.io/reddit/submission/search/?sort_type=created_utc&sort=asc&subreddit=amitheasshole&after='+ str(after) +\"&before\"+str(before)+\"&size=1000\"\n",
    "    print(url)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']\n",
    "\n",
    "timestamps = []\n",
    "post_ids = []\n",
    "score = []\n",
    "\n",
    "after = min_UTC\n",
    "while int(after) < last_UTC:\n",
    "    data = getPushshiftData(after,last_UTC)\n",
    "    for post in data:\n",
    "        tmp_time = post['created_utc']\n",
    "        tmp_id = post['id']\n",
    "        timestamps.append(tmp_time)\n",
    "        post_ids.append(tmp_id)\n",
    "    after = timestamps[-1]\n",
    "    print([str(len(post_ids)) + \" posts collected so far.\"])\n",
    "    time.sleep(0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.pushshift.io/reddit/submission/search/?sort_type=created_utc&sort=asc&subreddit=amitheasshole&after=1370649600&before1577836800&size=1000\n"
     ]
    }
   ],
   "source": [
    "list = getPushshiftData(1370649600, 1577836800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create and save separate csv files for post ids and created UTCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'id':post_ids, 'timestamp':timestamps}\n",
    "df = pd.DataFrame(d)\n",
    "df.to_csv(\"post_ids.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"post_ids.csv\")\n",
    "use_posts = df['id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1fy0bx</td>\n",
       "      <td>1370724175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ytr72</td>\n",
       "      <td>1393275159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1ytxov</td>\n",
       "      <td>1393278651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1yu29c</td>\n",
       "      <td>1393281184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1yu41e</td>\n",
       "      <td>1393282238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354948</th>\n",
       "      <td>eil3lv</td>\n",
       "      <td>1577900691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354949</th>\n",
       "      <td>eil42u</td>\n",
       "      <td>1577900755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354950</th>\n",
       "      <td>eil4jh</td>\n",
       "      <td>1577900812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354951</th>\n",
       "      <td>eil4lx</td>\n",
       "      <td>1577900820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354952</th>\n",
       "      <td>eil4ns</td>\n",
       "      <td>1577900827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>354953 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id   timestamp\n",
       "0       1fy0bx  1370724175\n",
       "1       1ytr72  1393275159\n",
       "2       1ytxov  1393278651\n",
       "3       1yu29c  1393281184\n",
       "4       1yu41e  1393282238\n",
       "...        ...         ...\n",
       "354948  eil3lv  1577900691\n",
       "354949  eil42u  1577900755\n",
       "354950  eil4jh  1577900812\n",
       "354951  eil4lx  1577900820\n",
       "354952  eil4ns  1577900827\n",
       "\n",
       "[354953 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354953\n"
     ]
    }
   ],
   "source": [
    "print(len(use_posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a datetime value counts plot with `matplotlib` gives us something similar to what we saw using the Reddit Search earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                  354953\n",
       "unique                 352942\n",
       "top       2019-12-17 21:05:22\n",
       "freq                        3\n",
       "first     2013-06-08 20:42:55\n",
       "last      2020-01-01 17:47:07\n",
       "Name: timestamp, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['timestamp'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x121854130>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['timestamp'].dt.date.value_counts().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Scraping with PRAW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a list of post ids from the dataframe we created in step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_posts = df['id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make sure we're requesting a read-only Reddit instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit.read_only = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354953\n"
     ]
    }
   ],
   "source": [
    "print(len(use_posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "f = open('subreddit_posts_out.csv',\"w\") \n",
    "writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "header = [\"id\",\"timestamp\",\"title\",\"body\",\"edited\",\"verdict\",\"score\",\"num_comments\"]\n",
    "writer.writerow(header)\n",
    "counter = 0\n",
    "for idx in use_posts:\n",
    "    post = reddit.submission(idx)\n",
    "\n",
    "    score = post.score\n",
    "    \n",
    "    if score >= 3:\n",
    "        title = post.title\n",
    "        body = post.selftext\n",
    "        edited = str(post.edited)\n",
    "        num_comments = post.num_comments\n",
    "        timestamp = post.created_utc\n",
    "        verdict = post.link_flair_text \n",
    "        if not verdict:\n",
    "            verdict =  \"NA\" \n",
    "            \n",
    "        line_stuff = [idx,timestamp,title,body,edited,verdict,score,num_comments]\n",
    "        writer.writerow(line_stuff)\n",
    "    else:\n",
    "        line_stuff = [idx,\"NA\",\"NA\",\"NA\",\"NA\",\"NA\",score,\"NA\"]\n",
    "        writer.writerow(line_stuff)\n",
    "\n",
    "    if counter % 1000 == 0:\n",
    "        print(counter)\n",
    "    counter += 1\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Cleaning and Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a function to clean the data we've collected. We'll start by useing the `lower` method on the `verdict` key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['verdict'] = df['verdict'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use the `replace` method to add some consistency to our verdicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['verdict'] = df['verdict'].str.replace(\"a--hole|a-hole\",\"asshole\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a `valid_list` so that our dataset only contains submissions that received valid verdicts.  We'll also use `replace` to remove any edits that could potentially spoil the verdict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_list = [\"asshole\",\"not the asshole\",\"everyone sucks\",\"no assholes here\"]\n",
    "df_use = df[df['verdict'].isin(valid_list)]\n",
    "df_use['body'] = df_use['body'].str.replace(\"(edit|update).*?(YTA|a-|ass|\\\\sta\\\\s)(.*)\",\"\",case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate a `gone_list` to get rid of posts that have been removed or deleted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gone_list = [\"[deleted]\",\"[removed]\",\"\"]\n",
    "df_use = df_use[df_use['body'].isin(gone_list)==False]\n",
    "print(\"After removing deleted posts, there are \" +  str(len(df_use)) + \" posts left.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since r/AmItheAsshole has four potential verdicts, we'll convert these into a binary variable.  Here, \"asshole\" and \"everyone sucks here\" will both be considered `is_asshole`, while all other verdicts will not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_use[\"is_asshole\"] = [1 if x in [\"asshole\",\"everyone sucks\"] else 0 for x in df_use[\"verdict\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altogether, our `clean_scrape` function will look like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_scrape(df):\n",
    "\n",
    "    df['verdict'] = df['verdict'].str.lower()\n",
    "\n",
    "    df['verdict'] = df['verdict'].str.replace(\"a--hole|a-hole\",\"asshole\") \n",
    "\n",
    "    valid_list = [\"asshole\",\"not the asshole\",\"everyone sucks\",\"no assholes here\"]\n",
    "    \n",
    "    df_use = df[df['verdict'].isin(valid_list)]\n",
    "    \n",
    "    df_use['body'] = df_use['body'].str.replace(\"(edit|update).*?(YTA|a-|ass|\\\\sta\\\\s)(.*)\",\"\",case=False)\n",
    "\n",
    "    gone_list = [\"[deleted]\",\"[removed]\",\"\"]\n",
    "    \n",
    "    df_use = df_use[df_use['body'].isin(gone_list)==False]\n",
    "    \n",
    "    print(\"After removing deleted posts, there are \" +  str(len(df_use)) + \" posts left.\")\n",
    "    \n",
    "    df_use[\"is_asshole\"] = [1 if x in [\"asshole\",\"everyone sucks\"] else 0 for x in df_use[\"verdict\"]]\n",
    "\n",
    "    return(df_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `concat` method from pandas to merge our old and new datasets.  We'll `drop_duplicates` from the concatenated dataset as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_scrape(old, new):\n",
    "    # old and new are pandas dataframes. should have the same columns\n",
    "    old = pd.concat([old,new])\n",
    "    old = old.drop_duplicates()\n",
    "    return(old)\n",
    "\n",
    "raw = pd.read_csv(\"aita_raw.csv\")\n",
    "grand = clean_scrape(raw)\n",
    "\n",
    "print(\"There are now \" +  str(len(grand)) + \" cleaned posts.\")\n",
    "\n",
    "grand.to_csv(\"aita_clean.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll use our `clean_scrape` function on the newly concatenated datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv(\"aita_raw.csv\")\n",
    "grand = clean_scrape(raw)\n",
    "\n",
    "print(\"There are now \" +  str(len(grand)) + \" cleaned posts.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
