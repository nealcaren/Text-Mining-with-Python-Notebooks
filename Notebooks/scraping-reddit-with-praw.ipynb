{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study, Part 2: Scraping Reddit with PRAW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although Pushshift is a wonderful resource when it comes to scraping Reddit data, it's not infallible. In some cases, important data will be missing from the Pushshift API, and you'll need to supplement the Pushshift data with the metadata available through Reddit's official API. \n",
    "\n",
    "Luckily, we can accomplish this using the [PRAW](https://praw.readthedocs.io/en/latest/) Reddit API Wrapper. This chapter will go through the steps necessary to supplement Pushshift data using PRAW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in /opt/anaconda3/lib/python3.7/site-packages (7.0.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /opt/anaconda3/lib/python3.7/site-packages (from praw) (0.57.0)\n",
      "Requirement already satisfied: update-checker>=0.16 in /opt/anaconda3/lib/python3.7/site-packages (from praw) (0.17)\n",
      "Requirement already satisfied: prawcore<2.0,>=1.3.0 in /opt/anaconda3/lib/python3.7/site-packages (from praw) (1.3.0)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from websocket-client>=0.54.0->praw) (1.14.0)\n",
      "Requirement already satisfied: requests>=2.3.0 in /opt/anaconda3/lib/python3.7/site-packages (from update-checker>=0.16->praw) (2.22.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.16->praw) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.16->praw) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.16->praw) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.16->praw) (2.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Reddit App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use PRAW, you'll need to develop your own application on Reddit. In order to do *that*, you'll need to create a Reddit account. \n",
    "\n",
    "\n",
    "Once you've created an account on Reddit, you can navigate to the [developed applications](https://www.reddit.com/prefs/apps) page from Reddit preferences. Here, you'll see a button prompting you to \"create app.\" Click it, and you should see the following: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![create application](https://i.snipboard.io/zKZ3vq.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you're creating a **script** app, as this is what we'll need in order to make requests with PRAW. Feel free to name and describe the app as you see fit, then click the button at the bottom to create your app. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional guidance on how to develop your own Reddit application, see [here](https://github.com/reddit-archive/reddit/wiki/OAuth2-Quick-Start-Example#first-steps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining a Reddit Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created an application on Reddit, we can obtain Reddit instances using PRAW. While it's possible to create two separate types of instance -- read-only or authorized -- for the sake of this chapter we'll focus on obtaining a read-only instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the script application we just created becomes important. We'll need to provide our `client-id`, our `client_secret`, and our `user_agent` in order to obtain a read-only Reddit instance:\n",
    "\n",
    "*Note*: You'll want to keep this information as confidential as possible while still accessing the data you need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=\"my client id\",\n",
    "                     client_secret=\"my client secret\",\n",
    "                     user_agent=\"my user agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the information from our script application, we'll be able to `print` a read-only Reddit instance. As an example, below are 20 of the most \"hot\" submissions from [r/MakeupRehab](https://www.reddit.com/r/MakeupRehab/): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Megathread: COVID-19 / Coronavirus Resource and Discussion\n",
      "Topic Tuesday - 'What is Your Favourite Lip Product in Your Collection' ''- May 26, 2020\n",
      "Makeup Rehabbers.. and at what point did you feel fully rehabbed? What happens when you end a no buy? & Other questions..\n",
      "Use Your Perfume Samples!\n",
      "Tempted to get rid of most of my makeup. What should I do?\n",
      "Constantly obsessively feeling like my collection is incomplete due to changing needs/preferences.\n",
      "Limited Edition does not mean you have to buy it.\n",
      "Not makeup, hope this is ok.\n",
      "Talk me out of a purchase?\n",
      "Ways to use up products I had bought \"for fun\" and now regret?\n",
      "2 months of buying NO makeup!!!\n",
      "30 days WITHOUT buying any makeup/skincare!\n",
      "Feel like I'm forcing myself to shop at Sephora because I have a gift card\n",
      "MuR Daily Chat - May 26, 2020\n",
      "*May.25th: Daily check in thread for No Buy May (NBM)*\n",
      "Talk me out of buying Urban Decay Brow Blade\n",
      "Does your makeup expire?\n",
      "makeup companies and my little ponyâ€”how they sell us the same product every year\n",
      "Found a replacement product within my stash!\n",
      "Project Progress - May 25, 2020\n"
     ]
    }
   ],
   "source": [
    "print(reddit.read_only)\n",
    "for submission in reddit.subreddit(\"MakeupRehab\").hot(limit=20):\n",
    "    print(submission.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in gaps in the Pushshift API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Pushshift is a great entryway into scraping data from Reddit, you'll occasionally run into some notable gaps in available data. These gaps in the Pushshift data tend to turn up following a [large wave of subreddit quarantines in September 2018](https://www.newsweek.com/reddit-quarantine-subs-toxic-controversial-moderators-1144663).\n",
    "\n",
    "\n",
    "Using the API's [Reddit Search](https://redditsearch.io/) referenced at the end of Part 1, we can see how scope out any potential holes in subreddit data before going through the process of scraping that data ourselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's look at the data one of the subreddits quarantined in September 2018, r/CringeAnarchy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![redditsearch](https://i.snipboard.io/7qI8eF.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a fairly large chunk of data missing starting following the quarantine. Data availabiliy only picks back up at the beginning of 2019. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
